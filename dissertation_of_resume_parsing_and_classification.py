# -*- coding: utf-8 -*-
"""DISSERTATION OF RESUME_PARSING_AND_CLASSIFICATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vVmQ1CBMkFCCdMyt8NzGUePSclKOmyPY
"""

! pip install pypdf2
# Import necessary libraries
import pandas as pd
import re
import spacy
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Bidirectional, Embedding, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
import joblib
import re
from transformers import InputExample, InputFeatures
from torch.utils.data import Dataset, DataLoader
from transformers import DistilBertTokenizer
from torch.optim import AdamW
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
from tqdm import tqdm
import torch

# Load dataset
try:
    resume_data = pd.read_csv("Resume.csv")
    print("Dataset loaded successfully:")
    print(resume_data.head())
except FileNotFoundError:
    print("Error: Resume.csv not found. Please upload the file.")
    exit()

resume_data["Category"].value_counts()

# Define category mapping to 3 super-categories
category_mapping = {
    'HR': 'Business & Management',
    'DESIGNER': 'Creative & Services',
    'INFORMATION-TECHNOLOGY': 'Technology & Engineering',
    'TEACHER': 'Social & Education',
    'ADVOCATE': 'Creative & Services',
    'BUSINESS-DEVELOPMENT': 'Business & Management',
    'HEALTHCARE': 'Social & Education',
    'FITNESS': 'Social & Education',
    'AGRICULTURE': 'Social & Education',
    'BPO': 'Creative & Services',
    'SALES': 'Creative & Services',
    'CONSULTANT': 'Business & Management',
    'DIGITAL-MEDIA': 'Technology & Engineering',
    'AUTOMOBILE': 'Technology & Engineering',
    'CHEF': 'Creative & Services',
    'FINANCE': 'Business & Management',
    'APPAREL': 'Creative & Services',
    'ENGINEERING': 'Technology & Engineering',
    'ACCOUNTANT': 'Business & Management',
    'CONSTRUCTION': 'Creative & Services',
    'PUBLIC-RELATIONS': 'Business & Management',
    'BANKING': 'Business & Management',
    'ARTS': 'Creative & Services',
    'AVIATION': 'Creative & Services'
}

resume_data['Super_Category'] = resume_data['Category'].map(category_mapping)

# Preprocessing Functions
nlp = spacy.load("en_core_web_sm")

def clean_text(text):
    """
    Clean and normalize text by:
    - Converting to lowercase
    - Removing URLs
    - Removing email addresses
    - Removing special characters (keeping basic punctuation)
    - Removing extra whitespace
    - Handling empty strings
    - Normalizing unicode characters
    """
    if not isinstance(text, str) or not text.strip():
        return ""
    # Normalize unicode (convert fancy quotes to standard, etc.)
    text = text.encode('ascii','ignore').decode('ascii')
    text = text.lower()
    # Remove URLs and email addresses
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    # Remove special characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # Remove extra whitespace
    text = ' '.join(text.split())
    return text

# Apply preprocessing
resume_data['cleaned_text'] = resume_data['Resume_str'].apply(clean_text)

"""**DistilBERT-Based transformer model classification**


"""

# Encode labels
label_encoder = LabelEncoder()
resume_data['encoded_labels'] = label_encoder.fit_transform(resume_data['Super_Category'])

# Split data
train_texts, test_texts, train_labels, test_labels = train_test_split(
    resume_data['cleaned_text'], resume_data['encoded_labels'],
    test_size=0.15, random_state=42
)
train_texts, val_texts, train_labels, val_labels = train_test_split(
    train_texts, train_labels, test_size=0.2, random_state=42
)

# Create PyTorch Dataset
class ResumeDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]

        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
MAX_LENGTH = 128
BATCH_SIZE = 16

# Create datasets
train_dataset = ResumeDataset(train_texts, train_labels, tokenizer, MAX_LENGTH)
val_dataset = ResumeDataset(val_texts, val_labels, tokenizer, MAX_LENGTH)
test_dataset = ResumeDataset(test_texts, test_labels, tokenizer, MAX_LENGTH)

# Create dataloaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)

# Initialize model
model = DistilBertForSequenceClassification.from_pretrained(
    'distilbert-base-uncased',
    num_labels=len(label_encoder.classes_)
)

# Set up device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

# Training setup
EPOCHS = 3
optimizer = AdamW(model.parameters(), lr=3e-5)
total_steps = len(train_loader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
)

# Training function
def train_epoch(model, data_loader, optimizer, device, scheduler):
    model.train()
    total_loss = 0

    for batch in tqdm(data_loader, desc="Training"):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()
        scheduler.step()

    return total_loss / len(data_loader)

# Evaluation function
def eval_model(model, data_loader, device):
    model.eval()
    predictions = []
    true_labels = []
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(data_loader, desc="Evaluating"):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )

            loss = outputs.loss
            total_loss += loss.item()

            logits = outputs.logits
            _, preds = torch.max(logits, dim=1)

            predictions.extend(preds.cpu().tolist())
            true_labels.extend(labels.cpu().tolist())

    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, average='weighted')
    recall = recall_score(true_labels, predictions, average='weighted')
    f1 = f1_score(true_labels, predictions, average='weighted')

    return {
        'loss': total_loss / len(data_loader),
        'accuracy': accuracy,
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': confusion_matrix(true_labels, predictions)
    }

# Training loop
best_accuracy = 0
for epoch in range(EPOCHS):
    print(f"Epoch {epoch + 1}/{EPOCHS}")
    train_loss = train_epoch(model, train_loader, optimizer, device, scheduler)
    print(f"Train loss: {train_loss:.4f}")

    val_metrics = eval_model(model, val_loader, device)
    print(f"Validation metrics: {val_metrics}")

    # Save best model
    if val_metrics['accuracy'] > best_accuracy:
        best_accuracy = val_metrics['accuracy']
        torch.save(model.state_dict(), 'best_distilbert_model.pth')

# Load best model and evaluate on test set
model.load_state_dict(torch.load('best_distilbert_model.pth'))
test_metrics = eval_model(model, test_loader, device)

print("\nFinal Test Metrics:")
print(f"Accuracy: {test_metrics['accuracy']:.4f}")
print(f"Precision: {test_metrics['precision']:.4f}")
print(f"Recall: {test_metrics['recall']:.4f}")
print(f"F1-Score: {test_metrics['f1']:.4f}")
print("Confusion Matrix:")
print(test_metrics['confusion_matrix'])

"""* Vectorization, spliting, classifing data with lr_model and dt_model"""

# Text Vectorization for Traditional Models
tfidf = TfidfVectorizer(max_features=5000)
X_tfidf = tfidf.fit_transform(resume_data['cleaned_text'])
y = resume_data['Super_Category']

# Data Split with K-Fold Cross-Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
for train_index, test_index in kfold.split(X_tfidf):
    X_train_tfidf, X_test_tfidf = X_tfidf[train_index], X_tfidf[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    X_train_tfidf, X_val_tfidf, y_train, y_val = train_test_split(X_train_tfidf, y_train, test_size=0.2, random_state=42)
    break  # Use first fold for simplicity

# Traditional Models with Regularization
lr_model = LogisticRegression(max_iter=1000, random_state=42, C=1.0, penalty='l2')
dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=5)

lr_model.fit(X_train_tfidf, y_train)

dt_model.fit(X_train_tfidf, y_train)

y_pred_lr = lr_model.predict(X_test_tfidf)
y_pred_dt = dt_model.predict(X_test_tfidf)

# Evaluation Metrics for Traditional Models
print("\nLogistic Regression Metrics:")
print("Accuracy:", accuracy_score(y_test, y_pred_lr))
print("Precision:", precision_score(y_test, y_pred_lr, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_lr, average='weighted'))
print("F1-Score:", f1_score(y_test, y_pred_lr, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_lr))

print("\nDecision Tree Metrics:")
print("Accuracy:", accuracy_score(y_test, y_pred_dt))
print("Precision:", precision_score(y_test, y_pred_dt, average='weighted'))
print("Recall:", recall_score(y_test, y_pred_dt, average='weighted'))
print("F1-Score:", f1_score(y_test, y_pred_dt, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_dt))

# Bi-LSTM Model
max_words = 5000
max_len = 100

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(resume_data['cleaned_text'])
sequences = tokenizer.texts_to_sequences(resume_data['cleaned_text'])
X_padded = pad_sequences(sequences, maxlen=max_len)

X_train_blstm, X_test_blstm, y_train_blstm, y_test_blstm = train_test_split(X_padded, y, test_size=0.15, random_state=42)
X_train_blstm, X_val_blstm, y_train_blstm, y_val_blstm = train_test_split(X_train_blstm, y_train_blstm, test_size=0.2, random_state=42)

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train_blstm)
y_test_encoded = le.transform(y_test_blstm)
y_val_encoded = le.transform(y_val_blstm)

blstm_model = Sequential()
blstm_model.add(Embedding(max_words, 100, input_length=max_len))
blstm_model.add(Bidirectional(LSTM(64, return_sequences=True)))
blstm_model.add(Dropout(0.3))  # Increased dropout
blstm_model.add(Bidirectional(LSTM(32)))
blstm_model.add(Dropout(0.3))  # Increased dropout
blstm_model.add(Dense(len(le.classes_), activation='softmax'))

blstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
blstm_model.fit(X_train_blstm, y_train_encoded, validation_data=(X_val_blstm, y_val_encoded), epochs=20, batch_size=32, callbacks=[early_stopping])

y_pred_blstm = np.argmax(blstm_model.predict(X_test_blstm), axis=1)
y_pred_blstm = le.inverse_transform(y_pred_blstm)

# Evaluation Metrics for Bi-LSTM
print("\nBi-LSTM Metrics:")
print("Accuracy:", accuracy_score(y_test_blstm, y_pred_blstm))
print("Precision:", precision_score(y_test_blstm, y_pred_blstm, average='weighted'))
print("Recall:", recall_score(y_test_blstm, y_pred_blstm, average='weighted'))
print("F1-Score:", f1_score(y_test_blstm, y_pred_blstm, average='weighted'))
print("Confusion Matrix:\n", confusion_matrix(y_test_blstm, y_pred_blstm))

import matplotlib.pyplot as plt
import seaborn as sns

# Collect metrics
metrics = {
    'Logistic Regression': {
        'Accuracy': accuracy_score(y_test, y_pred_lr),
        'Precision': precision_score(y_test, y_pred_lr, average='weighted'),
        'Recall': recall_score(y_test, y_pred_lr, average='weighted'),
        'F1-Score': f1_score(y_test, y_pred_lr, average='weighted')
    },
    'Decision Tree': {
        'Accuracy': accuracy_score(y_test, y_pred_dt),
        'Precision': precision_score(y_test, y_pred_dt, average='weighted'),
        'Recall': recall_score(y_test, y_pred_dt, average='weighted'),
        'F1-Score': f1_score(y_test, y_pred_dt, average='weighted')
    },
    'Bi-LSTM': {
        'Accuracy': accuracy_score(y_test_blstm, y_pred_blstm),
        'Precision': precision_score(y_test_blstm, y_pred_blstm, average='weighted'),
        'Recall': recall_score(y_test_blstm, y_pred_blstm, average='weighted'),
        'F1-Score': f1_score(y_test_blstm, y_pred_blstm, average='weighted')
    },
    'DistilBERT': {
        'Accuracy': test_metrics['accuracy'],
        'Precision': test_metrics['precision'],
        'Recall': test_metrics['recall'],
        'F1-Score': test_metrics['f1']
    }
}

# Display metrics in a pandas DataFrame for easy comparison
metrics_df = pd.DataFrame(metrics).T
print("\nModel Performance Comparison:")
display(metrics_df)

print("Visualizing Model Accuracy Comparison:")

plt.figure(figsize=(10, 6))
sns.barplot(x=metrics_df.index, y=metrics_df['Accuracy'], palette='viridis')
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Predict the category of a new resume using the best-performing model
new_resume_text = """
Experienced software engineer with a strong background in Python and machine learning.
Skilled in developing and deploying scalable applications.
Bachelor's degree in Computer Science.
"""
# Cleaning new resume text using the defined function
cleaned_new_resume_text = clean_text(new_resume_text)
print(f"Cleaned new resume text: {cleaned_new_resume_text}")

# Re-initialize the DistilBertTokenizer to ensure we are using the correct one
distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

MAX_LENGTH = 512
new_resume_encoding = distilbert_tokenizer(
    cleaned_new_resume_text,
    truncation=True,
    padding='max_length',
    max_length=MAX_LENGTH,
    return_tensors='pt'
)

model.eval()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Make prediction with PyTorch
with torch.no_grad():
    input_ids = new_resume_encoding['input_ids'].to(device)
    attention_mask = new_resume_encoding['attention_mask'].to(device)

    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    logits = outputs.logits
    predicted_class_index = torch.argmax(logits, dim=1).item()

# Inverse transform the predicted class to the original category name
predicted_category = label_encoder.inverse_transform([predicted_class_index])[0]
print(f"\nPredicted Category for the new resume: {predicted_category}")

import os

# ------------------------------------------------------------------
# 1. Save model & tokenizer
# ------------------------------------------------------------------
save_dir = "distilbert_resume_classifier"
os.makedirs(save_dir, exist_ok=True)

model.save_pretrained(save_dir)                  # saves config + pytorch_model.bin
distilbert_tokenizer.save_pretrained(save_dir)              # saves vocab.txt + tokenizer.json
print(f"✅ Model & tokenizer saved to ./{save_dir}")

# ------------------------------------------------------------------
# 2. Save label encoder so the app knows the mapping
# ------------------------------------------------------------------
joblib.dump(label_encoder, os.path.join(save_dir, "label_encoder.pkl"))
print("✅ Label encoder saved")